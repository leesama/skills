import subprocess
import json
import os
import re
from datetime import datetime, timedelta
from collections import defaultdict

# é…ç½®
AUTHOR = ""   # ä¾‹å¦‚ "Alice" æˆ– "alice@example.com"
# AUTHOR ä¸ºç©ºæ—¶ï¼Œè‡ªåŠ¨ä½¿ç”¨æœ¬æœº git çš„ user.name/user.email
# ç»Ÿè®¡æ¨¡å¼ï¼š"week" æŒ‰å‘¨ç»Ÿè®¡ï¼Œ"month" æŒ‰æœˆç»Ÿè®¡
STAT_MODE = "week"  # "week" æˆ– "month"
# ä½¿ç”¨æŒ‰å‘¨ç»Ÿè®¡ï¼Œè€Œä¸æ˜¯æœ€è¿‘ N å¤©
WEEK_START = 0  # ä¸€å‘¨èµ·å§‹ï¼š0=å‘¨ä¸€, 6=å‘¨æ—¥
WEEK_OFFSET = 0  # 0=æœ¬å‘¨, 1=ä¸Šå‘¨, 2=ä¸Šä¸Šå‘¨...
# æŒ‰æœˆç»Ÿè®¡çš„åç§»é‡
MONTH_OFFSET = 0  # 0=æœ¬æœˆ, 1=ä¸Šæœˆ, 2=ä¸Šä¸Šæœˆ...
# è‡ªåŠ¨è¯†åˆ«ä»“åº“é…ç½®
# 1) REPO_ROOTSï¼šæœ¬åœ°ä»“åº“å­˜æ”¾çš„æ ¹ç›®å½•åˆ—è¡¨ï¼ˆä¼šåœ¨è¿™äº›ç›®å½•ä¸‹æ‰«æ Git ä»“åº“ï¼‰
# 2) COMPANY_GIT_PATTERNSï¼šå…¬å¸ Git åœ°å€çš„å…³é”®å­—ï¼ˆåªä¿ç•™ remote.url å‘½ä¸­è¿™äº›å…³é”®å­—çš„ä»“åº“ï¼‰
# 3) REPO_PATHSï¼šè‹¥æ˜¾å¼å¡«å†™åˆ™ç›´æ¥ä½¿ç”¨ï¼Œä¸å†è‡ªåŠ¨è¯†åˆ«ï¼ˆä¸ºç©ºè¡¨ç¤ºå¯ç”¨è‡ªåŠ¨è¯†åˆ«ï¼‰
REPO_ROOTS = []
COMPANY_GIT_PATTERNS = []  # ç•™ç©ºè¡¨ç¤ºä¸åšè¿œç¨‹åœ°å€è¿‡æ»¤
REPO_PATHS = []  # ä¸ºç©ºæ—¶è‡ªåŠ¨è¯†åˆ«
MAX_SCAN_DEPTH = 4  # æ‰«ææ·±åº¦ï¼Œé¿å…éå†è¿‡å¤šç›®å½•

CONFIG_ENV = "WEEKLY_REPORT_CONFIG"

def _normalize_list(value):
    if value is None:
        return []
    if isinstance(value, list):
        return [str(v).strip() for v in value if str(v).strip()]
    if isinstance(value, str):
        return [v.strip() for v in value.split(",") if v.strip()]
    return []

def _maybe_int(value, default):
    try:
        return int(value)
    except Exception:
        return default

def load_config():
    """ä»ç¯å¢ƒå˜é‡æˆ–é»˜è®¤ä½ç½®è¯»å–é…ç½®"""
    config_paths = [
        os.environ.get(CONFIG_ENV, "").strip(),
        os.path.join(os.getcwd(), "weekly.config.json"),
        os.path.expanduser("~/.config/weekly-report/config.json"),
        os.path.expanduser("~/.weekly-report.json"),
    ]
    for path in config_paths:
        if not path:
            continue
        if os.path.isfile(path):
            try:
                with open(path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                if isinstance(data, dict):
                    return data, path
            except Exception:
                pass
    return {}, ""

def apply_config(config):
    """å°†é…ç½®è¦†ç›–åˆ°å…¨å±€é»˜è®¤å€¼"""
    global AUTHOR, STAT_MODE, WEEK_START, WEEK_OFFSET, MONTH_OFFSET
    global REPO_ROOTS, COMPANY_GIT_PATTERNS, REPO_PATHS, MAX_SCAN_DEPTH

    if not isinstance(config, dict):
        return

    if "author" in config:
        AUTHOR = str(config.get("author") or "").strip()
    if "stat_mode" in config:
        STAT_MODE = str(config.get("stat_mode") or STAT_MODE).strip().lower() or STAT_MODE
    if "week_start" in config:
        WEEK_START = _maybe_int(config.get("week_start"), WEEK_START)
    if "week_offset" in config:
        WEEK_OFFSET = _maybe_int(config.get("week_offset"), WEEK_OFFSET)
    if "month_offset" in config:
        MONTH_OFFSET = _maybe_int(config.get("month_offset"), MONTH_OFFSET)
    if "repo_roots" in config:
        REPO_ROOTS = _normalize_list(config.get("repo_roots"))
    if "company_git_patterns" in config:
        COMPANY_GIT_PATTERNS = _normalize_list(config.get("company_git_patterns"))
    if "repo_paths" in config:
        REPO_PATHS = _normalize_list(config.get("repo_paths"))
    if "max_scan_depth" in config:
        MAX_SCAN_DEPTH = _maybe_int(config.get("max_scan_depth"), MAX_SCAN_DEPTH)

def _default_repo_roots():
    env = os.environ.get("WEEKLY_REPORT_REPO_ROOTS", "").strip()
    if env:
        if os.pathsep in env:
            return [p.strip() for p in env.split(os.pathsep) if p.strip()]
        return _normalize_list(env)
    return [os.getcwd()]



def get_week_range(week_offset: int = 0, week_start: int = 0):
    """è¿”å›æŒ‡å®šå‘¨çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDï¼‰
    week_offset: 0=æœ¬å‘¨, 1=ä¸Šå‘¨...
    week_start: 0=å‘¨ä¸€, 6=å‘¨æ—¥
    """
    today = datetime.now().date()
    weekday = today.weekday()  # å‘¨ä¸€=0, å‘¨æ—¥=6
    days_since_week_start = (weekday - week_start) % 7
    start_of_this_week = today - timedelta(days=days_since_week_start)
    start_date = start_of_this_week - timedelta(weeks=week_offset)
    end_date = start_date + timedelta(days=6)
    return start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")

def get_month_range(month_offset: int = 0):
    """è¿”å›æŒ‡å®šæœˆçš„å¼€å§‹å’Œç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDï¼‰
    month_offset: 0=æœ¬æœˆ, 1=ä¸Šæœˆ, 2=ä¸Šä¸Šæœˆ...
    """
    from calendar import monthrange
    today = datetime.now().date()
    
    # è®¡ç®—ç›®æ ‡æœˆä»½
    year = today.year
    month = today.month - month_offset
    
    # å¤„ç†è·¨å¹´çš„æƒ…å†µ
    while month <= 0:
        month += 12
        year -= 1
    while month > 12:
        month -= 12
        year += 1
    
    # è·å–è¯¥æœˆçš„ç¬¬ä¸€å¤©å’Œæœ€åä¸€å¤©
    start_date = datetime(year, month, 1).date()
    _, last_day = monthrange(year, month)
    end_date = datetime(year, month, last_day).date()
    
    return start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")

def iter_git_repos(root, max_depth=4):
    """åœ¨æŒ‡å®šæ ¹ç›®å½•ä¸‹æŸ¥æ‰¾ Git ä»“åº“è·¯å¾„"""
    root = os.path.abspath(root)
    for dirpath, dirnames, _ in os.walk(root):
        depth = dirpath[len(root):].count(os.sep)
        if depth > max_depth:
            dirnames[:] = []
            continue
        if ".git" in dirnames:
            yield dirpath
            dirnames[:] = []
            continue

def get_repo_remotes(repo_path):
    """è·å–ä»“åº“çš„è¿œç¨‹åœ°å€åˆ—è¡¨"""
    cmd = ["git", "-C", repo_path, "remote", "-v"]
    result = subprocess.run(cmd, stdout=subprocess.PIPE, text=True, stderr=subprocess.PIPE)
    remotes = set()
    if result.returncode == 0 and result.stdout:
        for line in result.stdout.splitlines():
            parts = line.split()
            if len(parts) >= 2:
                remotes.add(parts[1])
    return remotes

def is_company_repo(repo_path, git_patterns):
    """åˆ¤æ–­ä»“åº“ remote æ˜¯å¦åŒ¹é…å…¬å¸ Git åœ°å€å…³é”®å­—"""
    if not git_patterns:
        return True
    remotes = get_repo_remotes(repo_path)
    for url in remotes:
        for pattern in git_patterns:
            if pattern and pattern in url:
                return True
    return False

def discover_repo_paths(repo_roots, git_patterns, max_depth=4):
    """è‡ªåŠ¨è¯†åˆ«åŒ¹é…å…¬å¸ Git åœ°å€çš„ä»“åº“è·¯å¾„"""
    repo_paths = []
    for root in repo_roots:
        if not root or not os.path.isdir(root):
            continue
        for repo_path in iter_git_repos(root, max_depth=max_depth):
            if is_company_repo(repo_path, git_patterns):
                repo_paths.append(repo_path)
    return sorted(set(repo_paths))

def get_repo_paths():
    """è·å–ä»“åº“è·¯å¾„åˆ—è¡¨ï¼šä¼˜å…ˆä½¿ç”¨ REPO_PATHSï¼Œå¦åˆ™è‡ªåŠ¨è¯†åˆ«"""
    if REPO_PATHS:
        return REPO_PATHS
    roots = REPO_ROOTS or _default_repo_roots()
    if not roots:
        cwd = os.getcwd()
        if os.path.isdir(os.path.join(cwd, ".git")):
            return [cwd]
        return []
    return discover_repo_paths(roots, COMPANY_GIT_PATTERNS, MAX_SCAN_DEPTH)

def get_git_config_value(repo_path, key, use_global=False):
    """è¯»å– git é…ç½®å€¼"""
    if use_global:
        cmd = ["git", "config", "--global", key]
    else:
        cmd = ["git", "-C", repo_path, "config", key]
    result = subprocess.run(cmd, stdout=subprocess.PIPE, text=True, stderr=subprocess.PIPE)
    if result.returncode == 0:
        return result.stdout.strip()
    return ""

def resolve_author_pattern(repo_path, author):
    """AUTHOR ä¸ºç©ºæ—¶è‡ªåŠ¨ä½¿ç”¨ git user.name/user.email ç”Ÿæˆ author åŒ¹é…æ¨¡å¼"""
    if author and str(author).strip():
        pattern = str(author).strip()
        return pattern, ("|" in pattern)
    email = get_git_config_value(repo_path, "user.email") or get_git_config_value(repo_path, "user.email", use_global=True)
    name = get_git_config_value(repo_path, "user.name") or get_git_config_value(repo_path, "user.name", use_global=True)
    if email:
        return re.escape(email), False
    if name:
        return re.escape(name), False
    return "", False



def check_commit_in_branches(repo_path, commit_hash):
    """æ£€æŸ¥æäº¤æ˜¯å¦å­˜åœ¨äºç‰¹å®šåˆ†æ”¯ä¸­
    è¿”å›åˆ†æ”¯ç±»å‹ï¼šrelease, pre-test, feature, zsxr, unknown
    ä¼˜å…ˆçº§ï¼šrelease > zsxr > pre-test > feature
    """
    try:
        # æ£€æŸ¥æäº¤æ˜¯å¦åœ¨åˆ†æ”¯ä¸­
        cmd_pretest = ["git", "-C", repo_path, "branch", "--contains", commit_hash]
        result = subprocess.run(cmd_pretest, stdout=subprocess.PIPE, text=True, stderr=subprocess.PIPE)
        
        if result.returncode == 0 and result.stdout.strip():
            branches = result.stdout.strip().split('\n')
            branches = [b.strip().replace('*', '').strip() for b in branches]
            
            # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥ï¼šrelease > zsxr > pre-test > feature
            # æ£€æŸ¥æ˜¯å¦åŒ…å«releaseåˆ†æ”¯ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            for branch in branches:
                if 'release' in branch.lower():
                    return 'release'
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«zsxrå¼€å¤´çš„åˆ†æ”¯ï¼ˆå·²å®ŒæˆçŠ¶æ€ï¼‰
            for branch in branches:
                if branch.lower().startswith('zsxr'):
                    return 'zsxr'
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«pre-teståˆ†æ”¯
            for branch in branches:
                if 'pre-test' in branch.lower():
                    return 'pre-test'
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«featureåˆ†æ”¯
            for branch in branches:
                if 'feature' in branch.lower():
                    return 'feature'
            
            return 'other'
        else:
            return 'unknown'
    except Exception as e:
        return 'unknown'

def get_project_name_from_readme(repo_path):
    """ä» README.md è¯»å–ç¬¬ä¸€è¡Œä½œä¸ºé¡¹ç›®å"""
    possible_names = ["README.md", "readme.md", "Readme.md"]
    for name in possible_names:
        readme_path = os.path.join(repo_path, name)
        if os.path.exists(readme_path):
            try:
                with open(readme_path, 'r', encoding='utf-8') as f:
                    line = f.readline()
                    while line.startswith('#'):
                        line = line.lstrip('#').strip()
                    if line:
                        return line.strip()
            except Exception:
                pass
    return None

def get_git_commits(author, repo_paths, start_date, end_date):
    """ä»å¤šä¸ªä»“åº“è·å–Gitæäº¤è®°å½•ï¼ˆæŒ‰å‘¨èŒƒå›´ï¼‰"""
    since_arg = f"{start_date} 00:00:00"
    until_arg = f"{end_date} 23:59:59"
    all_commits = []
    
    for repo_path in repo_paths:
        try:
            author_filter, use_extended = resolve_author_pattern(repo_path, author)
            cmd = [
                "git", "-C", repo_path, "log",
                "--all",  # æ‰«ææ‰€æœ‰åˆ†æ”¯
                f"--since={since_arg}",
                f"--until={until_arg}",
                "--pretty=format:%ad | %s | %H",  # æ—¥æœŸã€æäº¤ä¿¡æ¯å’Œæäº¤å“ˆå¸Œ
                "--date=short"
            ]
            if author_filter:
                cmd.insert(6, f"--author={author_filter}")
                if use_extended:
                    cmd.insert(6, "--extended-regexp")
            result = subprocess.run(cmd, stdout=subprocess.PIPE, text=True, stderr=subprocess.PIPE)
            
            if result.returncode == 0 and result.stdout.strip():
                # æ£€æŸ¥ README å¹¶è·å–é¡¹ç›®å
                project_name = get_project_name_from_readme(repo_path)
                if not project_name:
                    raise ValueError(f"ä»“åº“ {repo_path} æœ¬å‘¨æœ‰æäº¤ä½†æœªæ‰¾åˆ° README.mdï¼Œæ— æ³•æå–é¡¹ç›®åï¼")
                
                repo_name = project_name
                commits = result.stdout.strip().split("\n")
                # è¿‡æ»¤æ‰åˆå¹¶ç›¸å…³çš„æäº¤ï¼Œåªä¿ç•™æœ¬äººçš„ç›´æ¥æäº¤
                filtered_commits = []
                for c in commits:
                    # è¿‡æ»¤æ‰å„ç§åˆå¹¶ç›¸å…³çš„æäº¤
                    if ("Merge branch" not in c and 
                        "Merge pull request" not in c and
                        "Merge remote-tracking branch" not in c and
                        "merge" not in c.lower()):
                        
                        # è§£ææäº¤ä¿¡æ¯ï¼šæ—¥æœŸ | æ¶ˆæ¯ | æäº¤å“ˆå¸Œ
                        parts = c.split(" | ")
                        if len(parts) >= 3:
                            date, msg, commit_hash = parts[0], parts[1], parts[2]
                            
                            # è¿›ä¸€æ­¥éªŒè¯è¿™æ˜¯æœ¬äººçš„ç›´æ¥æäº¤ï¼ˆéåˆå¹¶æäº¤ï¼‰
                            # æ£€æŸ¥æäº¤æ˜¯å¦æœ‰å¤šä¸ªçˆ¶æäº¤ï¼ˆåˆå¹¶æäº¤çš„ç‰¹å¾ï¼‰
                            parent_check_cmd = ["git", "-C", repo_path, "rev-list", "--parents", "-n", "1", commit_hash]
                            parent_result = subprocess.run(parent_check_cmd, stdout=subprocess.PIPE, text=True, stderr=subprocess.PIPE)
                            
                            if parent_result.returncode == 0:
                                parents = parent_result.stdout.strip().split()
                                # å¦‚æœæœ‰è¶…è¿‡2ä¸ªçˆ¶æäº¤ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰ï¼Œè¯´æ˜æ˜¯åˆå¹¶æäº¤ï¼Œè·³è¿‡
                                if len(parents) <= 2:  # è‡ªå·± + 1ä¸ªçˆ¶æäº¤ = æ­£å¸¸æäº¤
                                    # æ£€æŸ¥è¯¥æäº¤æ˜¯å¦å­˜åœ¨äºå„ä¸ªåˆ†æ”¯
                                    branch_status = check_commit_in_branches(repo_path, commit_hash)
                                    # æ·»åŠ ä»“åº“åå’Œåˆ†æ”¯çŠ¶æ€
                                    filtered_commits.append(f"{date} | {msg} | [{repo_name}] | {branch_status}")
                        elif len(parts) >= 2:
                            date, msg = parts[0], parts[1]
                            # æ²¡æœ‰æäº¤å“ˆå¸Œçš„æƒ…å†µï¼Œä¿å®ˆå¤„ç†
                            filtered_commits.append(f"{date} | {msg} | [{repo_name}] | unknown")
                all_commits.extend(filtered_commits)
            else:
                # print(f"âš ï¸  ä»“åº“ {repo_path} è·å–æäº¤è®°å½•å¤±è´¥æˆ–æ— æäº¤è®°å½•")
                pass
                
        except Exception as e:
            print(f"âŒ å¤„ç†ä»“åº“ {repo_path} æ—¶å‡ºé”™: {e}")
            # å¦‚æœæ˜¯ ValueError (README ç¼ºå¤±), è¿™é‡Œçš„ print å¯èƒ½ä¸å¤Ÿï¼Œä½†ç”¨æˆ·è¦æ±‚"æŠ¥é”™"ã€‚
            # è¿™é‡Œçš„ print(..., error) å·²ç»ç®—æ˜¯æŠ¥é”™äº†ã€‚
            # è„šæœ¬ä¼šç»§ç»­æ‰§è¡Œå…¶ä»–ä»“åº“ã€‚
    
    # æŒ‰æ—¥æœŸæ’åºæ‰€æœ‰æäº¤
    all_commits.sort(key=lambda x: x.split(" | ")[0], reverse=True)
    return all_commits

def count_commits_by_date(commits):
    count_map = defaultdict(int)
    for c in commits:
        date, _ = c.split(" | ", 1)
        count_map[date] += 1
    # æŒ‰æ—¥æœŸæ’åºè¿”å›
    return dict(sorted(count_map.items()))

def clean_commit_message(message):
    """æ¸…ç†æäº¤ä¿¡æ¯ï¼Œå»é™¤å‰ç¼€"""
    # å¸¸è§çš„æäº¤å‰ç¼€
    prefixes = ['feat:', 'feature:', 'fix:', 'docs:', 'style:', 'refactor:', 'perf:', 'test:', 'chore:', 'build:', 'ci:']
    
    for prefix in prefixes:
        if message.startswith(prefix):
            return message[len(prefix):].strip()
    
    return message.strip()

def normalize_message_for_dedup(message):
    """è§„èŒƒåŒ–æ¶ˆæ¯ç”¨äºå»é‡ï¼ˆå»é™¤æœ«å°¾æ•°å­—/ç¬¦å·ï¼‰"""
    if not message:
        return message
    normalized = re.sub(r'\d+$', '', message).strip()
    normalized = re.sub(r'\d+[^\w]*$', '', normalized).strip()
    return normalized

def deduplicate_similar_messages(messages):
    """å»é‡ç›¸ä¼¼çš„æäº¤ä¿¡æ¯ï¼ˆåªæ˜¯æœ«å°¾æ•°å­—ä¸åŒçš„ï¼‰ï¼Œå¹¶å»é™¤æœ«å°¾æ•°å­—"""
    if not messages:
        return messages
    
    # ç”¨äºå­˜å‚¨å»é‡åçš„æ¶ˆæ¯
    unique_messages = []
    seen_patterns = set()
    
    for msg in messages:
        # å°†æœ«å°¾çš„æ•°å­—æ›¿æ¢ä¸ºå ä½ç¬¦ï¼Œç”¨äºæ¯”è¾ƒç›¸ä¼¼æ€§
        pattern = normalize_message_for_dedup(msg)
        
        if pattern not in seen_patterns:
            seen_patterns.add(pattern)
            # è¿”å›å»é™¤æœ«å°¾æ•°å­—åçš„æ¶ˆæ¯
            unique_messages.append(pattern)
    
    return unique_messages

def process_commits_to_tasks(commits):
    """å°†Gitæäº¤è®°å½•è½¬æ¢ä¸ºä»»åŠ¡æ ¼å¼"""
    tasks = []
    
    if not commits:
        # å¦‚æœæ²¡æœ‰æäº¤è®°å½•ï¼Œæ·»åŠ ç¤ºä¾‹ä»»åŠ¡
        tasks.append([
            "",
            "", 
            "",
            "",  # å¤‡æ³¨æ ç•™ç©º
            ""   # ä»“åº“ä¿¡æ¯æ ï¼Œç”¨äºåç»­æ·»åŠ å‰ç¼€
        ])
    else:
        # æŒ‰æ—¥æœŸåˆ†ç»„æäº¤
        commits_by_date = defaultdict(list)
        for commit in commits:
            date = commit.split(" | ")[0]
            commits_by_date[date].append(commit)
        
        # ä¸ºæ¯ä¸ªæäº¤è®°å½•åˆ›å»ºå•ç‹¬çš„ä»»åŠ¡æ¡ç›®
        for date in sorted(commits_by_date.keys(), reverse=True):
            date_commits = commits_by_date[date]
            
            # æå–å¹¶æ¸…ç†æäº¤ä¿¡æ¯ï¼ŒåŒæ—¶è·å–ä»“åº“ä¿¡æ¯å’Œåˆ†æ”¯çŠ¶æ€
            commit_data = []
            for commit in date_commits:
                parts = commit.split(" | ")
                if len(parts) >= 4:
                    msg = clean_commit_message(parts[1])  # æ¸…ç†æäº¤ä¿¡æ¯å‰ç¼€
                    repo_name = parts[2].strip("[]")
                    branch_status = parts[3].strip() if parts[3].strip() else "unknown"
                    commit_data.append((msg, repo_name, branch_status))
                elif len(parts) >= 3:
                    msg = clean_commit_message(parts[1])
                    repo_name = parts[2].strip("[]")
                    branch_status = "unknown"
                    commit_data.append((msg, repo_name, branch_status))
            
            # åŒé¡¹ç›®ç›¸åŒ commit-msg åªä¿ç•™ä¸€æ¡
            grouped = defaultdict(list)
            ordered_keys = []
            seen_keys = set()
            
            for msg, repo_name, branch_status in commit_data:
                normalized_msg = normalize_message_for_dedup(msg)
                key = (repo_name, normalized_msg)
                grouped[key].append(branch_status or "unknown")
                if key not in seen_keys:
                    seen_keys.add(key)
                    ordered_keys.append(key)
            
            # ä¸ºæ¯ä¸ªå»é‡åçš„æäº¤ä¿¡æ¯åˆ›å»ºå•ç‹¬çš„ä»»åŠ¡è¡Œ
            for repo_name, msg in ordered_keys:
                if msg:  # åªå¤„ç†éç©ºæ¶ˆæ¯
                    # é€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„åˆ†æ”¯çŠ¶æ€
                    priority_order = {'release': 1, 'zsxr': 2, 'pre-test': 3, 'feature': 4, 'other': 5, 'unknown': 6}
                    statuses = grouped.get((repo_name, msg), ["unknown"])
                    statuses.sort(key=lambda x: priority_order.get(x, 7))
                    branch_status = statuses[0] if statuses else "unknown"
                    
                    # æ ¹æ®åˆ†æ”¯çŠ¶æ€åˆ¤æ–­å®ŒæˆçŠ¶æ€
                    status = get_task_status_by_branch(branch_status)
                    
                    # ç¿»è¯‘è‹±æ–‡æäº¤ä¿¡æ¯
                    task_content = msg
                    completion_standard = "å®Œæˆå¼€å‘å¹¶æäº¤"
                    notes = ""  # å¤‡æ³¨æ ç•™ç©º
                    repo_info = repo_name  # ä¿å­˜ä»“åº“ä¿¡æ¯
                    
                    tasks.append([task_content, completion_standard, status, notes, repo_info])
    
    return tasks

def analyze_commits_for_stats(commits):
    """é¢„ç»Ÿè®¡ï¼šå»é‡æ•°é‡"""
    if not commits:
        return 0
    unique_keys = set()
    for commit in commits:
        parts = commit.split(" | ")
        if len(parts) >= 3:
            msg = clean_commit_message(parts[1])
            repo_name = parts[2].strip("[]")
            normalized_msg = normalize_message_for_dedup(msg)
            key = (repo_name, normalized_msg)
            if key in unique_keys:
                continue
            unique_keys.add(key)
    return len(unique_keys)

def get_task_status_by_branch(branch_status):
    """æ ¹æ®åˆ†æ”¯çŠ¶æ€åˆ¤æ–­ä»»åŠ¡å®ŒæˆçŠ¶æ€"""
    if not branch_status or branch_status == 'unknown':
        return "å·²å®Œæˆ"  # é»˜è®¤çŠ¶æ€
    
    # æ ¹æ®åˆ†æ”¯çŠ¶æ€ç›´æ¥åˆ¤æ–­
    if branch_status == 'pre-test':
        return "æµ‹è¯•ä¸­"
    elif branch_status == 'release':
        return "å·²å®Œæˆ"
    elif branch_status == 'zsxr':
        return "å·²å®Œæˆ"
    elif branch_status == 'feature':
        return "å¾…æµ‹è¯•"
    else:
        return "å·²å®Œæˆ"

def final_deduplicate_tasks(tasks):
    """å¯¹æ‰€æœ‰ä»»åŠ¡è¿›è¡Œæœ€ç»ˆå»é‡ï¼ŒåŒ…æ‹¬è·¨è¡Œå»é‡ï¼Œå¹¶æ·»åŠ ä»“åº“å‰ç¼€"""
    if not tasks:
        return tasks
    
    # æå–ä»»åŠ¡å†…å®¹å’Œä»“åº“ä¿¡æ¯ï¼Œå¹¶æŒ‰ä»“åº“å»é‡
    task_data = []
    ordered_keys = []
    seen_keys = set()
    
    for task in tasks:
        if task and len(task) >= 5 and task[0]:  # ç¡®ä¿æœ‰ä»“åº“ä¿¡æ¯
            content = normalize_message_for_dedup(task[0])
            repo_name = task[4] if len(task) > 4 else ""
            key = (repo_name, content)
            task_data.append((key, task))
            if key not in seen_keys:
                seen_keys.add(key)
                ordered_keys.append(key)
    
    # é‡å»ºä»»åŠ¡åˆ—è¡¨ï¼Œæ·»åŠ ä»“åº“å‰ç¼€
    deduplicated_tasks = []
    for repo_name, content in ordered_keys:
        if content:  # åªä¿ç•™éç©ºå†…å®¹
            # æ‰¾åˆ°æ‰€æœ‰åŒ¹é…çš„ä»»åŠ¡ï¼Œé€‰æ‹©çŠ¶æ€ä¼˜å…ˆçº§æœ€é«˜çš„
            matched_tasks = [t for k, t in task_data if k == (repo_name, content)]
            if matched_tasks:
                # æŒ‰ä»»åŠ¡çŠ¶æ€ä¼˜å…ˆçº§æ’åºï¼šå·²å®Œæˆ > æµ‹è¯•ä¸­ > å¾…æµ‹è¯•
                status_priority = {'å·²å®Œæˆ': 1, 'æµ‹è¯•ä¸­': 2, 'å¾…æµ‹è¯•': 3}
                matched_tasks.sort(key=lambda x: status_priority.get(x[2] if len(x) > 2 else 'å·²å®Œæˆ', 4))
                best_task = matched_tasks[0]
                
                # æ ¹æ®ä»“åº“åæ·»åŠ å‰ç¼€
                prefix = f"ã€{repo_name}ã€‘ "
                new_task_content = prefix + content
                
                # æ„å»ºæ–°ä»»åŠ¡ï¼Œåªä¿ç•™å‰4åˆ—
                new_task = [
                    new_task_content,
                    best_task[1] if len(best_task) > 1 else "å®Œæˆå¼€å‘å¹¶æäº¤",
                    best_task[2] if len(best_task) > 2 else "å·²å®Œæˆ",
                    best_task[3] if len(best_task) > 3 else "",
                    repo_name
                ]
                deduplicated_tasks.append(new_task)
    
    return deduplicated_tasks

def save_tasks_to_json(tasks, start_date, end_date, total_commits, period_type="å‘¨"):
    """å°†ä»»åŠ¡æ•°æ®ä¿å­˜ä¸ºJSONæ ¼å¼"""
    # æ„å»ºJSONæ•°æ®ç»“æ„
    json_data = {
        "report_type": f"{period_type}æŠ¥",
        "period": {
            "start_date": start_date,
            "end_date": end_date
        },
        "statistics": {
            "total_commits": total_commits,
            "total_tasks": len(tasks),
            "completed": sum(1 for t in tasks if len(t) > 2 and t[2] == "å·²å®Œæˆ"),
            "testing": sum(1 for t in tasks if len(t) > 2 and t[2] == "æµ‹è¯•ä¸­"),
            "pending_test": sum(1 for t in tasks if len(t) > 2 and t[2] == "å¾…æµ‹è¯•")
        },
        "tasks": []
    }
    
    # ä¸´æ—¶å­˜å‚¨æŒ‰é¡¹ç›®åˆ†ç»„çš„ä»»åŠ¡
    grouped_tasks = defaultdict(list)
    
    # æ·»åŠ ä»»åŠ¡è¯¦æƒ…
    for task in tasks:
        task_item = {
            "content": task[0] if len(task) > 0 else "",
            "completion_standard": task[1] if len(task) > 1 else "",
            "status": task[2] if len(task) > 2 else "",
            "notes": task[3] if len(task) > 3 else ""
        }
        # è·å–é¡¹ç›®å (ç¬¬5åˆ—)
        project_name = task[4] if len(task) > 4 else "å…¶ä»–é¡¹ç›®"
        
        task_item["project_name"] = project_name
        grouped_tasks[project_name].append(task_item)
    
    # å°†åˆ†ç»„åçš„ä»»åŠ¡å†™å…¥ JSON
    json_data["projects"] = []
    for project_name, project_tasks in grouped_tasks.items():
        json_data["projects"].append({
            "project_name": project_name,
            "tasks": project_tasks
        })
        # åŒæ—¶ä¹Ÿä¿ç•™æ‰å¹³çš„ tasks åˆ—è¡¨ä»¥ä¾¿é€šè¿‡æ—§æ–¹å¼æŸ¥çœ‹ï¼ˆå¯é€‰ï¼Œè¿™é‡Œæˆ‘ä¿ç•™äº†æ‰å¹³åˆ—è¡¨ï¼Œæˆ–è€…å¯ä»¥æ¸…ç©ºï¼‰
        json_data["tasks"].extend(project_tasks)
    
    # ä¿å­˜JSONæ–‡ä»¶
    json_file = f"æœ¬{period_type}å·¥ä½œ{period_type}æŠ¥_{end_date}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… JSONæ•°æ®å·²ç”Ÿæˆï¼š{json_file}")
    return json_data

if __name__ == "__main__":
    config, config_path = load_config()
    if config:
        apply_config(config)
    if config_path:
        print(f"ğŸ”§ å·²åŠ è½½é…ç½®ï¼š{config_path}")

    repo_paths = get_repo_paths()
    if not repo_paths:
        print("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…å…¬å¸ Git åœ°å€çš„ä»“åº“ï¼Œè¯·æ£€æŸ¥ REPO_ROOTS å’Œ COMPANY_GIT_PATTERNS é…ç½®ã€‚")
    print(f"ğŸ“‚ æ­£åœ¨æ‰«æ {len(repo_paths)} ä¸ªä»“åº“...")
    for i, repo in enumerate(repo_paths, 1):
        print(f"   {i}. {repo}")
    print()

    # æ ¹æ®ç»Ÿè®¡æ¨¡å¼è®¡ç®—æ—¥æœŸèŒƒå›´
    if STAT_MODE == "month":
        start_date, end_date = get_month_range(MONTH_OFFSET)
        period_type = "æœˆ"
    else:
        start_date, end_date = get_week_range(WEEK_OFFSET, WEEK_START)
        period_type = "å‘¨"
    print(f"ğŸ—“ï¸ ç»Ÿè®¡æ¨¡å¼ï¼šæŒ‰{period_type}ç»Ÿè®¡")
    print(f"ğŸ—“ï¸ ç»Ÿè®¡åŒºé—´ï¼š{start_date} è‡³ {end_date}")
    
    commits = get_git_commits(AUTHOR, repo_paths, start_date, end_date)
    
    # ç»Ÿè®¡æ¯æ—¥æäº¤æ•°å¹¶è¾“å‡º
    count_map = count_commits_by_date(commits)
    total_commits = len(commits)
    print(f"ğŸ“Š æœ¬{period_type}å…±æ‰¾åˆ° {total_commits} æ¡æäº¤è®°å½•")
    for date, count in count_map.items():
        print(f"ğŸ“… {date}ï¼š{count} æ¡")
    # é¢„ç»Ÿè®¡å»é‡ä¸ç¿»è¯‘æ¡æ•°ï¼ˆåœ¨è€—æ—¶å¤„ç†å‰å…ˆæç¤ºï¼‰
    unique_tasks_count = analyze_commits_for_stats(commits)
    duplicates_removed = max(0, total_commits - unique_tasks_count)
    print(f"ğŸ§¹ å»é‡æäº¤ï¼šç§»é™¤ {duplicates_removed} æ¡é‡å¤è®°å½•ï¼ˆä» {total_commits} æ¡åˆå¹¶ä¸º {unique_tasks_count} æ¡ï¼‰")
    
    # å¤„ç†æäº¤è®°å½•ä¸ºä»»åŠ¡
    final_tasks = process_commits_to_tasks(commits)
    final_tasks = final_deduplicate_tasks(final_tasks)

    
    # åŒæ—¶ç”ŸæˆJSONæ•°æ®
    save_tasks_to_json(final_tasks, start_date, end_date, len(commits), period_type)
